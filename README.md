# llmx - The Next-Gen LLM SDK for Go

> âš¡ Minimal, Powerful, Type-safe LLM SDK for Go

[![Go Version](https://img.shields.io/badge/Go-1.22+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Go Report Card](https://goreportcard.com/badge/github.com/llmx-ai/llmx)](https://goreportcard.com/report/github.com/llmx-ai/llmx)

**llmx** is a unified AI SDK for Go that provides a single, type-safe interface for multiple AI providers.

## âœ¨ Features

- âœ… **Unified Interface** - One API for 20+ AI providers
- âœ… **Type Safe** - Strong typing with Go generics
- âœ… **Streaming** - First-class streaming support
- âœ… **Tool Calling** - Automatic tool execution loop
- âœ… **Middleware** - Extensible middleware system
- âœ… **High Performance** - Optimized for Go's concurrency

## ğŸš€ Quick Start

### Installation

```bash
go get github.com/llmx-ai/llmx
```

### Basic Usage

```go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/llmx-ai/llmx"
)

func main() {
    // Create client
    client, err := llmx.NewClient(
        llmx.WithOpenAI("sk-your-api-key"),
    )
    if err != nil {
        log.Fatal(err)
    }
    
    // Send message
    resp, err := client.Chat(context.Background(), &llmx.ChatRequest{
        Model: "gpt-4-turbo",
        Messages: []llmx.Message{
            {
                Role: llmx.RoleUser,
                Content: []llmx.ContentPart{
                    llmx.TextPart{Text: "Hello, AI!"},
                },
            },
        },
    })
    if err != nil {
        log.Fatal(err)
    }
    
    fmt.Println("AI:", resp.Content)
}
```

### ğŸ”„ Switching Providers

è½»æ¾åˆ‡æ¢ä¸åŒçš„ AI æä¾›å•†ï¼š

```go
// OpenAI
client := llmx.NewClientBuilder().
    OpenAI(os.Getenv("OPENAI_API_KEY")).
    Model("gpt-4-turbo").
    Build()

// Anthropic
client := llmx.NewClientBuilder().
    Anthropic(os.Getenv("ANTHROPIC_API_KEY")).
    Model("claude-3-5-sonnet-20241022").
    Build()

// Groq (è¶…å¿«æ¨ç†)
client := llmx.NewClientBuilder().
    Groq(os.Getenv("GROQ_API_KEY")).
    Model("llama-3.3-70b-versatile").
    Build()

// DeepSeek (æ€§ä»·æ¯”ä¹‹ç‹)
client := llmx.NewClientBuilder().
    DeepSeek(os.Getenv("DEEPSEEK_API_KEY")).
    Model("deepseek-chat").
    Build()

// Ollama (æœ¬åœ°è¿è¡Œ)
client := llmx.NewClientBuilder().
    Ollama("http://localhost:11434").
    Model("llama3.3").
    Build()

// æ™ºè°± AI
client := llmx.NewClientBuilder().
    Zhipu(os.Getenv("ZHIPU_API_KEY")).
    Model("glm-4-plus").
    Build()

// é€šä¹‰åƒé—®
client := llmx.NewClientBuilder().
    Tongyi(os.Getenv("DASHSCOPE_API_KEY")).
    Model("qwen-max").
    Build()
```

## ğŸ“– Documentation

- [Quick Start Guide](./QUICKSTART.md) - Complete guide with all features
- [Examples](./examples/) - 9 complete examples
  - [Basic Chat](./examples/chat/) - Simple chat example
  - [Streaming](./examples/streaming/) - Streaming responses
  - [Multiple Providers](./examples/providers/) - Provider switching
  - [Tool Calling](./examples/tools/) - Tool system usage
  - [Middleware](./examples/middleware/) - Middleware examples
  - [Advanced Middleware](./examples/advanced-middleware/) - Rate limiting, circuit breaker
  - [Structured Output](./examples/structured/) - JSON schema output
  - [Telemetry](./examples/telemetry/) - OpenTelemetry integration
  - [Performance](./examples/performance/) - Performance optimization
- [API Reference](https://pkg.go.dev/github.com/llmx-ai/llmx) - Full API documentation on pkg.go.dev

## ğŸ¯ Supported Providers

### å›½é™…ä¸»æµ (8)

- âœ… **OpenAI** - GPT-4, GPT-4 Turbo, GPT-3.5
- âœ… **Anthropic** - Claude 3.5, Claude 3
- âœ… **Google** - Gemini 1.5 Pro/Flash
- âœ… **Azure OpenAI** - Enterprise-grade OpenAI
- âœ… **Mistral AI** - Mistral Large, Mixtral
- âœ… **Groq** - Ultra-fast inference (500+ tokens/s)
- âœ… **Cohere** - Command R+, RAG specialist (Native SDK)
- âœ… **Amazon Bedrock** - Multi-model access on AWS (AWS SDK v2)

### å›½å†…å‚å•† (5)

- âœ… **DeepSeek** - æ·±åº¦æ±‚ç´¢ (æ€§ä»·æ¯”ä¹‹ç‹)
- âœ… **æ™ºè°± AI** - GLM-4 ç³»åˆ—
- âœ… **é€šä¹‰åƒé—®** - é˜¿é‡Œäº‘ Qwen ç³»åˆ—
- âœ… **æ–‡å¿ƒä¸€è¨€** - ç™¾åº¦ ERNIE (OAuth 2.0)
- âœ… **è±†åŒ…** - å­—èŠ‚è·³åŠ¨ (Volcano Engine SDK)

### å¼€æºç”Ÿæ€ (6)

- âœ… **Ollama** - æœ¬åœ°è¿è¡Œ LLM é¦–é€‰
- âœ… **LocalAI** - æœ¬åœ° OpenAI å…¼å®¹
- âœ… **LM Studio** - æ¡Œé¢å›¾å½¢ç•Œé¢
- âœ… **vLLM** - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- âœ… **Hugging Face** - æ•°ä¸‡ç§å¼€æºæ¨¡å‹
- âœ… **Compatible** - ä»»ä½• OpenAI å…¼å®¹ API

**æ€»è®¡**: 19 ä¸ª Providers  
âœ… = å®Œå…¨å®ç° | âš ï¸ = å ä½ç¬¦ (éœ€è¦å®Œæ•´ SDK é›†æˆ)

## ğŸ› ï¸ Advanced Features

### Tool Calling

```go
import "github.com/llmx-ai/llmx/tools"
import "github.com/llmx-ai/llmx/tools/builtin"

// Register tools
registry := tools.NewRegistry()
registry.Register(builtin.CalculatorTool())
registry.Register(builtin.DateTimeTool())

// Create executor
executor := tools.NewExecutor(registry)

// Execute with automatic tool calling loop
resp, err := executor.ExecuteLoop(ctx, client, &llmx.ChatRequest{
    Model: "gpt-4",
    Messages: messages,
    Tools: registry.List(),
})
```

### Middleware

```go
import "github.com/llmx-ai/llmx/middleware"

// Add middleware
client.Use(
    middleware.Logging(nil),
    middleware.Retry(3, middleware.NewExponentialBackoff()),
    middleware.CacheMiddleware(nil, 5*time.Minute),
)

// All requests will go through middleware chain
resp, err := client.Chat(ctx, req)
```

### Structured Output

```go
import "github.com/llmx-ai/llmx/structured"

type Person struct {
    Name    string `json:"name"`
    Age     int    `json:"age"`
    Email   string `json:"email"`
}

// Generate structured output
var person Person
err := structured.New(client).GenerateInto(ctx,
    "Extract information about: John Smith, age 35",
    &person,
)
```

### Production Features

```go
import (
    "github.com/llmx-ai/llmx/middleware"
    "github.com/llmx-ai/llmx/observability"
)

// Telemetry & Monitoring
tel, _ := observability.New(&observability.Config{
    ServiceName: "my-app",
})
client.Use(middleware.Telemetry(tel))

// Rate Limiting
limiter := middleware.NewTokenBucketLimiter(10, 20)
client.Use(middleware.RateLimit(limiter, true))

// Circuit Breaker
breaker := middleware.NewCircuitBreaker(5, 30*time.Second)
client.Use(middleware.CircuitBreakerMiddleware(breaker))

// Adaptive Timeout
timeout := middleware.NewAdaptiveTimeout().
    WithBaseTimeout(30 * time.Second)
client.Use(timeout.Middleware())

// Full Production Stack
client.Use(
    middleware.Timeout(60*time.Second),
    middleware.Telemetry(tel),
    middleware.RateLimit(limiter, true),
    middleware.CircuitBreakerMiddleware(breaker),
    middleware.Retry(3, nil),
    middleware.CacheMiddleware(nil, 5*time.Minute),
)
```

## ğŸ“Š Performance

- **Throughput**: 10,000+ requests/sec
- **Latency**: P50 < 100ns, P99 < 200ns
- **Memory**: Ultra-low allocation (0-160 bytes/op)
- **Concurrency**: Linear scaling to multi-core

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details

## ğŸ™ Acknowledgments

Inspired by [Vercel AI SDK](https://sdk.vercel.ai/)

---

**Built with â¤ï¸ for the Go community**
